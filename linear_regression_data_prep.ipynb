{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "import scipy.io as spio\n",
    "\n",
    "\n",
    "def loadmat(filename):\n",
    "    '''\n",
    "    this function should be called instead of direct spio.loadmat\n",
    "    as it cures the problem of not properly recovering python dictionaries\n",
    "    from mat files. It calls the function check keys to cure all entries\n",
    "    which are still mat-objects\n",
    "    '''\n",
    "    def _check_keys(d):\n",
    "        '''\n",
    "        checks if entries in dictionary are mat-objects. If yes\n",
    "        todict is called to change them to nested dictionaries\n",
    "        '''\n",
    "        for key in d:\n",
    "            if isinstance(d[key], spio.matlab.mio5_params.mat_struct):\n",
    "                d[key] = _todict(d[key])\n",
    "        return d\n",
    "\n",
    "    def _todict(matobj):\n",
    "        '''\n",
    "        A recursive function which constructs from matobjects nested dictionaries\n",
    "        '''\n",
    "        d = {}\n",
    "        for strg in matobj._fieldnames:\n",
    "            elem = matobj.__dict__[strg]\n",
    "            if isinstance(elem, spio.matlab.mio5_params.mat_struct):\n",
    "                d[strg] = _todict(elem)\n",
    "            elif isinstance(elem, np.ndarray):\n",
    "                d[strg] = _tolist(elem)\n",
    "            else:\n",
    "                d[strg] = elem\n",
    "        return d\n",
    "\n",
    "    def _tolist(ndarray):\n",
    "        '''\n",
    "        A recursive function which constructs lists from cellarrays\n",
    "        (which are loaded as numpy ndarrays), recursing into the elements\n",
    "        if they contain matobjects.\n",
    "        '''\n",
    "        elem_list = []\n",
    "        for sub_elem in ndarray:\n",
    "            if isinstance(sub_elem, spio.matlab.mio5_params.mat_struct):\n",
    "                elem_list.append(_todict(sub_elem))\n",
    "            elif isinstance(sub_elem, np.ndarray):\n",
    "                elem_list.append(_tolist(sub_elem))\n",
    "            else:\n",
    "                elem_list.append(sub_elem)\n",
    "        return elem_list\n",
    "    data = scipy.io.loadmat(filename, struct_as_record=False, squeeze_me=True)\n",
    "    return _check_keys(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homedir='/media/raghuram/My Passport/dicom_seg/TCGA-LGG'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(homedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicom_file_params_df=pd.read_csv('dicom_file_params.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicom_file_params_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_post_samples_df = pd.read_csv('t1_post_samples.csv')\n",
    "t1_pre_samples_df = pd.read_csv('t1_pre_samples.csv')\n",
    "flair_samples = pd.read_csv('flair_samples.csv')\n",
    "t2_pre_samples = pd.read_csv('t2_pre_samples.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t1_post_samples_df['Sequence Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_mapper_dict = {'T1CE':t1_post_samples_df.values,\n",
    "               'T1W':t1_pre_samples_df.values,\n",
    "               'T2W': t2_pre_samples.values,\n",
    "               'T2F': flair_samples.values}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_sequence = []\n",
    "for idx, row in dicom_file_params_df.iterrows():\n",
    "    try:\n",
    "        if row['scanning_seq_mri'] in sequence_mapper_dict['T1CE']:\n",
    "            mapped_sequence.append('T1CE')\n",
    "        elif row['scanning_seq_mri'] in sequence_mapper_dict['T1W']:\n",
    "            mapped_sequence.append('T1W')\n",
    "        elif row['scanning_seq_mri'] in sequence_mapper_dict['T2W']:\n",
    "            mapped_sequence.append('T2W')\n",
    "        elif row['scanning_seq_mri'] in sequence_mapper_dict['T2F']:\n",
    "            mapped_sequence.append('T2F')\n",
    "    except Exception as e:\n",
    "        print('{}, {} in row {}'.format(e, row['scanning_seq_mri'], idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicom_file_params_df['mat_file_sequence'] = mapped_sequence\n",
    "dicom_file_params_df.to_csv('dicom_file_params.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_name_list = []\n",
    "for idx, row in dicom_file_params_df.iterrows():\n",
    "    try:\n",
    "        patient_name_list.append(row['filename'].split('/TCGA-LGG')[1].split('/')[1])\n",
    "    except Exception as e:\n",
    "        print('Error {} at index {}'.format(e, idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicom_file_params_df['patient_name'] = patient_name_list\n",
    "dicom_file_params_df.to_csv('dicom_file_params.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_files_dir = '/home/raghuram/Desktop/radiomics/TEXTURES'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(mat_files_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_files_list = glob.glob('*.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loadmat(mat_files_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_files_list[-1 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicom_file_params_df['mat_file_name'] = dicom_file_params_df['patient_name']+'_'+dicom_file_params_df['mat_file_sequence']+'.mat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicom_file_params_df.drop_duplicates(subset=['mat_file_name'], keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicom_file_params_df.drop(columns=['patient_name'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicom_file_params_df.to_csv('dicom_file_params_df_with_mat_files.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = set(mat_files_list).difference(list(dicom_file_params_df['mat_file_name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_files_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicom_file_params_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_flatten_features(mat_data, filename):\n",
    "    \n",
    "    features_flattened = []\n",
    "    \n",
    "    for experiment_, values in data['textures']['List'].items():\n",
    "            \n",
    "        experiment_number = int(experiment_.split('Experiment')[1])\n",
    "        if experiment_number > 25:\n",
    "            break\n",
    "        scale_ = float(values.split(',')[0].split('=')[1])\n",
    "        algo_ = values.split(',')[1].split('=')[1]\n",
    "        ng_ = int(values.split(',')[2].split('=')[1])\n",
    "        \n",
    "        flattened_df = pd.io.json.json_normalize(data['textures'][experiment_], sep='_')\n",
    "        flattened_df['mat_file_name'] = filename\n",
    "        flattened_df_merged = pd.merge(flattened_df, dicom_file_params_df, on='mat_file_name', how='inner')\n",
    "        flattened_df_merged['experiment_number'] = experiment_number\n",
    "        flattened_df_merged['scale'] = scale_\n",
    "        flattened_df_merged['algo'] = algo_\n",
    "        flattened_df_merged['ng'] = ng_\n",
    "        features_flattened.append(flattened_df_merged)\n",
    "    \n",
    "    features_df_concat = pd.concat(features_flattened, ignore_index=True)\n",
    "    features_df_concat.to_csv(filename.split('.')[0]+'_features'+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# extract_flatten_features(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, mat_file in enumerate(mat_files_list):\n",
    "    \n",
    "    if mat_file in missing:\n",
    "            print('Filename not in dicom df, skipping')\n",
    "            continue   \n",
    "    \n",
    "    print('Mat file {} at index {} of total length {} being processed'.format(mat_file, idx, len(mat_files_list)))\n",
    "    data = loadmat(mat_file)\n",
    "    extract_flatten_features(data, mat_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

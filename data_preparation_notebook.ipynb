{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "import scipy.io as spio\n",
    "\n",
    "\n",
    "def loadmat(filename):\n",
    "    '''\n",
    "    this function should be called instead of direct spio.loadmat\n",
    "    as it cures the problem of not properly recovering python dictionaries\n",
    "    from mat files. It calls the function check keys to cure all entries\n",
    "    which are still mat-objects\n",
    "    '''\n",
    "    def _check_keys(d):\n",
    "        '''\n",
    "        checks if entries in dictionary are mat-objects. If yes\n",
    "        todict is called to change them to nested dictionaries\n",
    "        '''\n",
    "        for key in d:\n",
    "            if isinstance(d[key], spio.matlab.mio5_params.mat_struct):\n",
    "                d[key] = _todict(d[key])\n",
    "        return d\n",
    "\n",
    "    def _todict(matobj):\n",
    "        '''\n",
    "        A recursive function which constructs from matobjects nested dictionaries\n",
    "        '''\n",
    "        d = {}\n",
    "        for strg in matobj._fieldnames:\n",
    "            elem = matobj.__dict__[strg]\n",
    "            if isinstance(elem, spio.matlab.mio5_params.mat_struct):\n",
    "                d[strg] = _todict(elem)\n",
    "            elif isinstance(elem, np.ndarray):\n",
    "                d[strg] = _tolist(elem)\n",
    "            else:\n",
    "                d[strg] = elem\n",
    "        return d\n",
    "\n",
    "    def _tolist(ndarray):\n",
    "        '''\n",
    "        A recursive function which constructs lists from cellarrays\n",
    "        (which are loaded as numpy ndarrays), recursing into the elements\n",
    "        if they contain matobjects.\n",
    "        '''\n",
    "        elem_list = []\n",
    "        for sub_elem in ndarray:\n",
    "            if isinstance(sub_elem, spio.matlab.mio5_params.mat_struct):\n",
    "                elem_list.append(_todict(sub_elem))\n",
    "            elif isinstance(sub_elem, np.ndarray):\n",
    "                elem_list.append(_tolist(sub_elem))\n",
    "            else:\n",
    "                elem_list.append(sub_elem)\n",
    "        return elem_list\n",
    "    data = scipy.io.loadmat(filename, struct_as_record=False, squeeze_me=True)\n",
    "    return _check_keys(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydicom import dcmread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd '/home/raghuram/Desktop/radiomics/STUDIES/LGG_study/WORKSPACE/TCGA_DATA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAIR_data = loadmat('T2Fpath.mat')\n",
    "T1W_data = loadmat('T1Wpath.mat')\n",
    "T1CE_data = loadmat('T1CEpath.mat')\n",
    "T2W_data = loadmat('T2Wpath.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/media/raghuram/My Passport')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('dicom_filenames.csv', names=['filename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_path = [FLAIR_data['T2Fpath'], T1W_data['T1Wpath'],\n",
    "               T1CE_data['T1CEpath'], T2W_data['T2Wpath']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1ce_path = T1CE_data['T1CEpath']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flair_path = FLAIR_data['T2Fpath']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2w_path = T2W_data['T2Wpath']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1w_path = T1W_data['T1Wpath']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_dict(sequence_dict, sequence_path, sequence_name):\n",
    "    for idx, instanceid in enumerate(sequence_path):\n",
    "        if not isinstance(instanceid, str):\n",
    "            continue\n",
    "        sequence_dict[instanceid] = sequence_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_dict(sequence_dict, t1ce_path, 'T1CE')\n",
    "write_to_dict(sequence_dict, t1w_path, 'T1W')\n",
    "write_to_dict(sequence_dict, t2w_path, 'T2W')\n",
    "write_to_dict(sequence_dict, flair_path, 'T2F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_magnetic_strength(data):\n",
    "    \n",
    "    try:\n",
    "        mag_field_strength = data.MagneticFieldStrength\n",
    "        if mag_field_strength > 1000:\n",
    "            mag_field_strength /= 10000\n",
    "        return mag_field_strength\n",
    "    \n",
    "    except Exception:\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_patient_name(filename):\n",
    "    try:\n",
    "        return filename.split('/')[6]\n",
    "    except Exception:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_scanner_model_name(data):\n",
    "    try:\n",
    "        return data.ManufacturerModelName\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_scanner_model_manufacturer(data):\n",
    "    try:\n",
    "        return data.Manufacturer\n",
    "    except: \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_sequence_name(seriesinstanceUID):\n",
    "    try:\n",
    "        return sequence_dict.get(seriesinstanceUID, 'NA')\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_seriesinstance_uid(data):\n",
    "    try:\n",
    "        return data.SeriesInstanceUID\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_flip_angle(data):\n",
    "    try:\n",
    "        return data.FlipAngle\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_inversion_time(data):\n",
    "    try:\n",
    "        return data.InversionTime\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_repetition_time(data):\n",
    "    try:\n",
    "        return data.RepetitionTime\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_excitation_time(data):\n",
    "    try:\n",
    "        return data.EchoTime\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_name = []\n",
    "magnetic_strength_list = []\n",
    "scanner_model_list = []\n",
    "scanner_manufacturer_list = []\n",
    "patient_list = []\n",
    "series_list = []\n",
    "flip_angle_list = []\n",
    "repetition_time = []\n",
    "excitation_time = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if index%1000 == 0:\n",
    "        print('{} entries processed'.format(index+1))\n",
    "    try:\n",
    "        data = dcmread(row['filename'])\n",
    "        scanner_model_list.append(fetch_scanner_model_name(data))\n",
    "        scanner_manufacturer_list.append(fetch_scanner_model_manufacturer(data))\n",
    "        patient_list.append(fetch_patient_name(row['filename']))\n",
    "        magnetic_strength_list.append(fetch_magnetic_strength(data))\n",
    "        series_list.append(fetch_seriesinstance_uid(data))\n",
    "        sequence_name.append(fetch_sequence_name(data.SeriesInstanceUID))\n",
    "        flip_angle_list.append(fetch_flip_angle(data))\n",
    "        repetition_time.append(fetch_repetition_time(data))\n",
    "        excitation_time.append(fetch_excitation_time(data))\n",
    "    except Exception as e:\n",
    "        print('Error {} at index {}'.format(e ,index))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['seriesinstanceuid'] = series_list\n",
    "df['scanner_model'] = scanner_model_list\n",
    "df['scanner_manufacturer'] = scanner_manufacturer_list\n",
    "df['flip_angle'] = flip_angle_list\n",
    "df['patient_name'] = patient_list\n",
    "df['mag_field_strength'] = magnetic_strength_list\n",
    "df['sequence_name'] = sequence_name\n",
    "df['repetition_time'] = repetition_time\n",
    "df['excitation_time'] = excitation_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset=['seriesinstanceuid', 'sequence_name'], inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace({'General Electric':'GE', 'GE MEDICAL SYSTEMS':'GE', 'SIEMENS':'Siemens', 'Philips Healthcare': 'Philips', \n",
    "           'Philips Medical Systems': 'Philips'}, inplace=True)\n",
    "df = df[df['scanner_manufacturer'] != 'Hitachi Medical Corporation']\n",
    "df['mat_file_name'] = df['patient_name']+'_'+df['sequence_name']+'.mat'\n",
    "# df.drop(columns=['filename', 'seriesinstanceuid'], inplace=True)\n",
    "df.to_csv('mapped_files_to_sequences.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/media/raghuram/My Passport')\n",
    "import pandas as pd\n",
    "df = pd.read_csv('mapped_files_to_sequences.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_T1CE = df[df['sequence_name'] == 'T1CE']\n",
    "T1CE_files = list(df_T1CE['mat_file_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/raghuram/Desktop/radiomics/TEXTURES/mat_folder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "T1CE_mat_list = glob.glob('*_T1CE.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_flatten_features(data, filename):\n",
    "    \n",
    "    features_flattened = []\n",
    "    \n",
    "    for experiment_, values in data['textures']['List'].items():\n",
    "        \n",
    "        experiment_number = int(experiment_.split('Experiment')[1])\n",
    "        if experiment_number > 25:\n",
    "            break\n",
    "        scale_ = float(values.split(',')[0].split('=')[1])\n",
    "        algo_ = values.split(',')[1].split('=')[1]\n",
    "        ng_ = int(values.split(',')[2].split('=')[1])\n",
    "        \n",
    "        flattened_df = pd.io.json.json_normalize(data['textures'][experiment_], sep='_')\n",
    "        flattened_df['mat_file_name'] = filename\n",
    "        flattened_df_merged = pd.merge(flattened_df, df, on='mat_file_name', how='inner')\n",
    "        flattened_df_merged['experiment_number'] = experiment_number\n",
    "        flattened_df_merged['scale'] = scale_\n",
    "        flattened_df_merged['algo'] = algo_\n",
    "        flattened_df_merged['ng'] = ng_\n",
    "        features_flattened.append(flattened_df_merged)\n",
    "    \n",
    "    features_df_concat = pd.concat(features_flattened, ignore_index=True)\n",
    "    features_df_concat.to_csv(filename.split('.')[0]+'_features'+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_texture_csv(sequence_mat_list, sequence_file_list):\n",
    "    common_files = set(sequence_file_list).intersection(set(sequence_mat_list))\n",
    "    print(len(common_files))\n",
    "    for idx, mat_file in enumerate(common_files):\n",
    "        print('Processing file number {}'.format(idx+1))\n",
    "        data = loadmat(mat_file)\n",
    "        extract_flatten_features(data, mat_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n",
      "Processing file number 1\n",
      "Processing file number 2\n",
      "Processing file number 3\n",
      "Processing file number 4\n",
      "Processing file number 5\n",
      "Processing file number 6\n",
      "Processing file number 7\n",
      "Processing file number 8\n",
      "Processing file number 9\n",
      "Processing file number 10\n",
      "Processing file number 11\n",
      "Processing file number 12\n",
      "Processing file number 13\n",
      "Processing file number 14\n",
      "Processing file number 15\n",
      "Processing file number 16\n",
      "Processing file number 17\n",
      "Processing file number 18\n",
      "Processing file number 19\n",
      "Processing file number 20\n",
      "Processing file number 21\n",
      "Processing file number 22\n",
      "Processing file number 23\n",
      "Processing file number 24\n",
      "Processing file number 25\n",
      "Processing file number 26\n",
      "Processing file number 27\n",
      "Processing file number 28\n",
      "Processing file number 29\n",
      "Processing file number 30\n",
      "Processing file number 31\n",
      "Processing file number 32\n",
      "Processing file number 33\n",
      "Processing file number 34\n",
      "Processing file number 35\n",
      "Processing file number 36\n",
      "Processing file number 37\n",
      "Processing file number 38\n",
      "Processing file number 39\n",
      "Processing file number 40\n",
      "Processing file number 41\n",
      "Processing file number 42\n",
      "Processing file number 43\n",
      "Processing file number 44\n",
      "Processing file number 45\n",
      "Processing file number 46\n",
      "Processing file number 47\n",
      "Processing file number 48\n",
      "Processing file number 49\n",
      "Processing file number 50\n",
      "Processing file number 51\n",
      "Processing file number 52\n",
      "Processing file number 53\n",
      "Processing file number 54\n",
      "Processing file number 55\n",
      "Processing file number 56\n",
      "Processing file number 57\n",
      "Processing file number 58\n",
      "Processing file number 59\n",
      "Processing file number 60\n",
      "Processing file number 61\n",
      "Processing file number 62\n",
      "Processing file number 63\n",
      "Processing file number 64\n",
      "Processing file number 65\n",
      "Processing file number 66\n",
      "Processing file number 67\n",
      "Processing file number 68\n",
      "Processing file number 69\n",
      "Processing file number 70\n",
      "Processing file number 71\n",
      "Processing file number 72\n",
      "Processing file number 73\n",
      "Processing file number 74\n",
      "Processing file number 75\n",
      "Processing file number 76\n",
      "Processing file number 77\n",
      "Processing file number 78\n",
      "Processing file number 79\n",
      "Processing file number 80\n",
      "Processing file number 81\n",
      "Processing file number 82\n",
      "Processing file number 83\n",
      "Processing file number 84\n",
      "Processing file number 85\n",
      "Processing file number 86\n",
      "Processing file number 87\n",
      "Processing file number 88\n",
      "Processing file number 89\n",
      "Processing file number 90\n",
      "Processing file number 91\n",
      "Processing file number 92\n",
      "Processing file number 93\n",
      "Processing file number 94\n",
      "Processing file number 95\n",
      "Processing file number 96\n",
      "Processing file number 97\n",
      "Processing file number 98\n",
      "Processing file number 99\n"
     ]
    }
   ],
   "source": [
    "form_texture_csv(T1CE_mat_list, T1CE_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1ce_files = glob.glob('*_T1CE_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "for t1ce_file in t2f_files:\n",
    "    df = pd.read_csv(t1ce_file)\n",
    "    df['Tumor'] = df['patient_name']\n",
    "    df.drop(columns=['filename', 'seriesinstanceuid', 'patient_name', 'parameters_Algo', 'parameters_Scale',\n",
    "                    'parameters_Ng'], inplace=True)\n",
    "    df_list.append(df)\n",
    "\n",
    "pd.concat(df_list).to_csv(os.path.join('/home/raghuram/Desktop/radiomics/TEXTURES', 'expt_t1ce.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
